
=== Learning linear identity for dimension = 8 ===

=== Epoch 500 ===
loss = 0.00000000
learning rate = 0.05000000
Absolute performance:
max abs err = 0.00005013
mean abs err = 0.00000358

=== Epoch 1000 ===
loss = 0.00000000
learning rate = 0.02500000
Absolute performance:
max abs err = 0.00000036
mean abs err = 0.00000009

After 1000 epochs (0:00:04 hh:mm:ss)
Learned parameters: [Parameter containing:
tensor([[ 0.6668,  0.1470,  0.7217, -0.1147, -0.2563,  0.1786,  0.6479,  0.0366],
        [ 0.1287,  0.9432, -0.2788,  0.0443,  0.0990, -0.0690, -0.2502, -0.0141],
        [ 0.0901, -0.0398,  0.8048,  0.0310,  0.0693, -0.0483, -0.1752, -0.0099],
        [-0.1292,  0.0570,  0.2799,  0.9555, -0.0994,  0.0692,  0.2512,  0.0142],
        [-0.1042,  0.0460,  0.2258, -0.0359,  0.9198,  0.0559,  0.2027,  0.0115],
        [ 0.0888, -0.0392, -0.1923,  0.0306,  0.0683,  0.9524, -0.1726, -0.0098],
        [ 0.3753, -0.1656, -0.8129,  0.1292,  0.2887, -0.2011,  0.2703, -0.0413],
        [ 0.1135, -0.0501, -0.2459,  0.0391,  0.0873, -0.0609, -0.2208,  0.9875]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 0.0128, -0.0049, -0.0035,  0.0050,  0.0040, -0.0034, -0.0144, -0.0044],
       device='cuda:0', requires_grad=True)]

=== Learning linear identity for dimension = 32 ===

=== Epoch 500 ===
loss = 0.00006152
learning rate = 0.05000000
Absolute performance:
max abs err = 0.06335238
mean abs err = 0.00202178

=== Epoch 1000 ===
loss = 0.00000000
learning rate = 0.02500000
Absolute performance:
max abs err = 0.00000107
mean abs err = 0.00000021

=== Epoch 1500 ===
loss = 0.00000000
learning rate = 0.01250000
Absolute performance:
max abs err = 0.00000060
mean abs err = 0.00000017

After 1500 epochs (0:00:05 hh:mm:ss)
Learned parameters: [Parameter containing:
tensor([[ 0.2669, -0.0186,  0.1554,  ..., -0.1897, -0.1364,  0.2820],
        [ 0.1735,  0.1030,  0.0814,  ...,  0.0530,  0.1024,  0.0225],
        [-0.0476,  0.0913,  0.2727,  ..., -0.0799,  0.2055,  0.2013],
        ...,
        [-0.0445, -0.1368, -0.0397,  ...,  0.2190, -0.2134,  0.2193],
        [-0.0239,  0.2023,  0.2306,  ..., -0.1205, -0.0220,  0.1105],
        [ 0.0007, -0.0675, -0.0743,  ...,  0.1030, -0.0275,  0.1055]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-0.0495,  0.0793,  0.1054, -0.2600, -0.0134,  0.0672,  0.1018, -0.0434,
        -0.0427,  0.1616,  0.0531,  0.0202,  0.0458,  0.0494,  0.0932,  0.2393,
        -0.3006, -0.0602, -0.1367,  0.1464, -0.1253, -0.1533, -0.0574,  0.1877,
        -0.0275, -0.1172, -0.0773,  0.0009, -0.2423,  0.0502,  0.2016,  0.1938],
       device='cuda:0', requires_grad=True)]

=== Learning linear identity for dimension = 256 ===

=== Epoch 500 ===
loss = 0.01907960
learning rate = 0.05000000
Absolute performance:
max abs err = 1.17411733
mean abs err = 0.05933260

=== Epoch 1000 ===
loss = 0.00000000
learning rate = 0.02500000
Absolute performance:
max abs err = 0.00000215
mean abs err = 0.00000041

=== Epoch 1500 ===
loss = 0.00000000
learning rate = 0.01250000
Absolute performance:
max abs err = 0.00000143
mean abs err = 0.00000027

=== Epoch 2000 ===
loss = 0.00000000
learning rate = 0.00625000
Absolute performance:
max abs err = 0.00000143
mean abs err = 0.00000019

=== Epoch 2500 ===
loss = 0.00000000
learning rate = 0.00312500
Absolute performance:
max abs err = 0.00000095
mean abs err = 0.00000013

After 2500 epochs (0:00:04 hh:mm:ss)
Learned parameters: [Parameter containing:
tensor([[ 0.0409, -0.0201, -0.0069,  ...,  0.0230, -0.0143, -0.0251],
        [ 0.0444, -0.0273, -0.0257,  ...,  0.0003,  0.0078, -0.0268],
        [-0.0054,  0.0865,  0.0623,  ..., -0.0351, -0.0155,  0.0278],
        ...,
        [ 0.0467,  0.0325, -0.0224,  ...,  0.0600, -0.0061,  0.0330],
        [ 0.0366, -0.0279,  0.0229,  ...,  0.0224, -0.0448, -0.0322],
        [-0.0450, -0.0091, -0.0696,  ..., -0.0148, -0.0185,  0.0525]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-0.0425,  0.0101,  0.0117, -0.0427,  0.0744, -0.0351, -0.0180,  0.0223,
         0.0787, -0.0736, -0.0091, -0.0261, -0.0633, -0.0498, -0.0171, -0.0562,
        -0.1369, -0.0456,  0.0114,  0.0203,  0.0516, -0.0612,  0.0473,  0.1058,
         0.0321, -0.0240, -0.0471,  0.0026, -0.0058, -0.0240, -0.0059, -0.0286,
        -0.0467, -0.0132, -0.0372, -0.0464, -0.0326,  0.0500,  0.0757,  0.0645,
         0.0174,  0.0917,  0.0282,  0.0267, -0.0618,  0.1985, -0.0636,  0.0419,
         0.0172, -0.1374,  0.0634,  0.0548,  0.0458,  0.0380, -0.0318,  0.0605,
         0.1629,  0.0465, -0.2038,  0.0078, -0.1631,  0.0563,  0.0394, -0.0589,
         0.0725, -0.0203, -0.0373, -0.0349,  0.0060,  0.0377,  0.0064,  0.0600,
        -0.0500, -0.0376,  0.0112, -0.0444, -0.0462, -0.0019,  0.0568,  0.0298,
        -0.0315, -0.0102, -0.0583,  0.0319, -0.0131, -0.0044,  0.1007,  0.0254,
         0.0099, -0.0476, -0.0426, -0.0687, -0.1354, -0.0139, -0.0215, -0.0198,
         0.0057, -0.0459,  0.0325,  0.0193, -0.0264, -0.1095,  0.0813,  0.0524,
        -0.0407, -0.0340,  0.0470, -0.0275,  0.0116, -0.0445, -0.0064, -0.0076,
         0.0368, -0.0070, -0.0101,  0.0370, -0.0162, -0.0143, -0.0341, -0.0554,
        -0.0414, -0.0575,  0.0700,  0.0350,  0.0133,  0.0751, -0.0450,  0.0404,
         0.0541,  0.0294, -0.0259, -0.0296, -0.0410, -0.0312,  0.0595, -0.0251,
        -0.0046, -0.0715, -0.0123,  0.0211,  0.0461, -0.0342,  0.0029,  0.0505,
         0.0204,  0.0006,  0.0735, -0.0474,  0.0598, -0.0247,  0.0473, -0.0121,
         0.0286,  0.0076, -0.0211,  0.0145, -0.0632,  0.0341, -0.0144, -0.0180,
         0.0049, -0.0452,  0.0353,  0.0317, -0.0376,  0.0186,  0.0446, -0.0255,
        -0.0035,  0.0449, -0.0304, -0.0291,  0.0268,  0.0415,  0.0747, -0.0073,
        -0.0210, -0.0057, -0.0390,  0.0150,  0.0588,  0.0307,  0.0512,  0.0098,
         0.0434, -0.0350, -0.0494, -0.1325,  0.0654,  0.0233, -0.0125, -0.1767,
        -0.0208, -0.0137,  0.0722,  0.0555,  0.0314,  0.0242, -0.0423, -0.0539,
        -0.0328,  0.0139, -0.0312,  0.0054,  0.0193,  0.0128,  0.0894, -0.0080,
         0.0229,  0.0170,  0.0494, -0.0165, -0.0061,  0.0147, -0.0326,  0.0557,
        -0.0348,  0.0346, -0.0749, -0.0190,  0.0529,  0.0435,  0.1021, -0.0028,
         0.0820, -0.0426, -0.0115, -0.1832,  0.0328, -0.0194, -0.1492, -0.0437,
        -0.0413, -0.0017,  0.0298, -0.0217,  0.0070, -0.0066,  0.0768, -0.0195,
        -0.0618,  0.0314,  0.1472,  0.1634, -0.0309, -0.2128, -0.0096,  0.0310,
         0.0305, -0.0333, -0.0003,  0.0274,  0.0111,  0.0110, -0.0870,  0.1873],
       device='cuda:0', requires_grad=True)]

=== Learning linear identity for dimension = 1024 ===

=== Epoch 500 ===
loss = 0.19588995
learning rate = 0.05000000
Absolute performance:
max abs err = 3.99670076
mean abs err = 0.27208585

=== Epoch 1000 ===
loss = 0.00000000
learning rate = 0.02500000
Absolute performance:
max abs err = 0.00000399
mean abs err = 0.00000066

=== Epoch 1500 ===
loss = 0.00000000
learning rate = 0.01250000
Absolute performance:
max abs err = 0.00000322
mean abs err = 0.00000038

=== Epoch 2000 ===
loss = 0.00000000
learning rate = 0.00625000
Absolute performance:
max abs err = 0.00000140
mean abs err = 0.00000021

=== Epoch 2500 ===
loss = 0.00000000
learning rate = 0.00312500
Absolute performance:
max abs err = 0.00000095
mean abs err = 0.00000015

After 2500 epochs (0:00:04 hh:mm:ss)
Learned parameters: [Parameter containing:
tensor([[ 5.2247e-03, -8.4376e-03,  2.5612e-02,  ...,  2.0652e-02,
          2.0361e-02,  2.7027e-03],
        [ 2.3926e-02, -1.3335e-02, -2.0124e-03,  ...,  1.6762e-02,
         -1.5503e-02,  5.5970e-03],
        [-1.4700e-05,  4.2995e-04,  1.6019e-02,  ...,  1.5276e-02,
         -2.1284e-02, -2.0752e-02],
        ...,
        [ 2.3635e-02,  2.3690e-02,  1.1394e-02,  ...,  2.4116e-02,
          1.5275e-02,  3.0745e-03],
        [ 1.6903e-01,  1.0434e-02,  1.6895e-02,  ..., -1.8869e-02,
         -2.2129e-02,  4.4383e-03],
        [ 2.1681e-02,  2.0593e-02,  3.9974e-03,  ..., -8.1672e-03,
         -1.1218e-03,  1.0630e-03]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 0.0145,  0.0274,  0.0177,  ..., -0.0279, -0.0257,  0.0392],
       device='cuda:0', requires_grad=True)]

=== Learning linear identity for dimension = 4096 ===

=== Epoch 500 ===
loss = 2.73514676
learning rate = 0.05000000
Absolute performance:
max abs err = 11.18242645
mean abs err = 1.14173651

=== Epoch 1000 ===
loss = 0.00000000
learning rate = 0.02500000
Absolute performance:
max abs err = 0.00000709
mean abs err = 0.00000106

=== Epoch 1500 ===
loss = 0.00000000
learning rate = 0.01250000
Absolute performance:
max abs err = 0.00000393
mean abs err = 0.00000053

=== Epoch 2000 ===
loss = 0.00000000
learning rate = 0.00625000
Absolute performance:
max abs err = 0.00000286
mean abs err = 0.00000029

=== Epoch 2500 ===
loss = 0.00000000
learning rate = 0.00312500
Absolute performance:
max abs err = 0.00000145
mean abs err = 0.00000018

=== Epoch 3000 ===
loss = 0.00000000
learning rate = 0.00156250
Absolute performance:
max abs err = 0.00000119
mean abs err = 0.00000013

=== Epoch 3500 ===
loss = 0.00000000
learning rate = 0.00078125
Absolute performance:
max abs err = 0.00000095
mean abs err = 0.00000009

After 3500 epochs (0:00:16 hh:mm:ss)
Learned parameters: [Parameter containing:
tensor([[ 2.4830e-03, -2.0368e-02, -1.4197e-02,  ..., -3.7308e-03,
         -1.0914e-02,  5.1244e-03],
        [ 1.4101e-01, -1.0584e-02,  1.6455e-02,  ..., -4.3518e-03,
         -1.6235e-02,  1.2018e-03],
        [-2.9399e-03,  3.9277e-03,  2.7173e-04,  ...,  1.0983e-02,
         -5.8106e-03, -3.0628e-03],
        ...,
        [-3.2600e-03, -6.8777e-03,  2.8791e-03,  ...,  3.4208e-03,
          1.3485e-04, -5.5671e-03],
        [-7.1380e-04, -4.6007e-03, -1.3821e-02,  ...,  5.9374e-03,
         -3.2892e-03,  5.6919e-03],
        [-1.3245e-02,  7.7371e-05, -1.9790e-03,  ...,  1.0520e-02,
         -9.6705e-03, -1.4027e-02]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([-0.0069,  0.0064,  0.0024,  ...,  0.0072, -0.0009, -0.0183],
       device='cuda:0', requires_grad=True)]
